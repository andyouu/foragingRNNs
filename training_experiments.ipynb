{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the impact of the number of steps per period on learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import libraries\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from forage_training import train_multiple_networks\n",
    "from forage_analysis import get_mean_perf_by_seq_len\n",
    "import ngym_foraging as ngym_f\n",
    "from ngym_foraging.wrappers import pass_reward, pass_action\n",
    "import gym\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# GLOBAL VARIABLES\n",
    "# check if GPU is available\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# name of the task on the neurogym library\n",
    "TASK = 'ForagingBlocks-v0'\n",
    "\n",
    "TRAINING_KWARGS = {'dt': 100,\n",
    "                   'lr': 1e-2,\n",
    "                   'batch_size': 16,\n",
    "                   'seq_len': 300,\n",
    "                   'TASK': TASK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters configuration\n",
    "env_seed = 123\n",
    "total_num_timesteps = 600000\n",
    "num_periods = total_num_timesteps // 300\n",
    "TRAINING_KWARGS['num_periods'] = num_periods\n",
    "# create folder to save data based on env seed\n",
    "# main_folder = 'C:/Users/saraf/OneDrive/Documentos/IDIBAPS/foraging RNNs/nets/'\n",
    "main_folder = '/home/molano/Dropbox/Molabo/foragingRNNs/' # '/home/molano/foragingRNNs_data/nets/'\n",
    "test_flag = ''\n",
    "filename = 'training_nets'+test_flag+'.csv'\n",
    "# Set up the task\n",
    "w_factor = 0.00001\n",
    "mean_ITI = 200\n",
    "max_ITI = 400\n",
    "fix_dur = 100\n",
    "dec_dur = 100\n",
    "blk_dur = 50\n",
    "probs = np.array([0.1, 0.9])\n",
    "env_kwargs = {'dt': TRAINING_KWARGS['dt'], 'probs': probs,\n",
    "              'blk_dur': blk_dur, 'timing':\n",
    "                  {'ITI': ngym_f.random.TruncExp(mean_ITI, 100, max_ITI),\n",
    "                    # mean, min, max\n",
    "                    'fixation': fix_dur, 'decision': dec_dur},\n",
    "                  # Decision period}\n",
    "                  'rewards': {'abort': 0., 'fixation': 0., 'correct': 1.}}\n",
    "TRAINING_KWARGS['classes_weights'] =\\\n",
    "    torch.tensor([w_factor*TRAINING_KWARGS['dt']/(mean_ITI),\n",
    "                  w_factor*TRAINING_KWARGS['dt']/fix_dur, 2, 2])\n",
    "# call function to sample\n",
    "env = gym.make(TASK, **env_kwargs)\n",
    "env = pass_reward.PassReward(env)\n",
    "env = pass_action.PassAction(env)\n",
    "# set seed\n",
    "env.seed(env_seed)\n",
    "env.reset()\n",
    "net_kwargs = {'hidden_size': 128,\n",
    "              'action_size': env.action_space.n,\n",
    "              'input_size': env.observation_space.shape[0]}\n",
    "TRAINING_KWARGS['env_kwargs'] = env_kwargs\n",
    "TRAINING_KWARGS['net_kwargs'] = net_kwargs\n",
    "\n",
    "# create folder to save data based on parameters\n",
    "save_folder = (f\"{main_folder}w{w_factor}_mITI{mean_ITI}_xITI{max_ITI}_f{fix_dur}_\"\n",
    "                f\"d{dec_dur}_nb{np.round(blk_dur/1e3, 1)}_\"\n",
    "                f\"prb{probs[0]}\")\n",
    "\n",
    "# Save config as npz\n",
    "np.savez(save_folder+'/config.npz', **TRAINING_KWARGS)\n",
    "\n",
    "num_steps_plot = 100\n",
    "num_steps_test = 10000\n",
    "debug = False\n",
    "num_networks = 4\n",
    "criterion = nn.CrossEntropyLoss(weight=TRAINING_KWARGS['classes_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Period:  199 of 12000\n",
      "mean performance:  0.0\n",
      "mean reward:  0.04\n",
      "Loss:  0.73329758644104\n",
      "------------\n",
      "Period:  399 of 12000\n",
      "mean performance:  0.0\n",
      "mean reward:  0.0\n",
      "Loss:  0.7974122166633606\n",
      "------------\n",
      "Period:  599 of 12000\n",
      "mean performance:  0.14285714285714285\n",
      "mean reward:  0.06\n",
      "Loss:  0.7669962048530579\n",
      "------------\n",
      "Period:  799 of 12000\n",
      "mean performance:  0.8571428571428571\n",
      "mean reward:  0.24\n",
      "Loss:  0.610235869884491\n",
      "------------\n",
      "Period:  999 of 12000\n",
      "mean performance:  0.0\n",
      "mean reward:  0.02\n",
      "Loss:  0.7824932932853699\n",
      "------------\n",
      "Period:  1199 of 12000\n",
      "mean performance:  0.0\n",
      "mean reward:  0.02\n",
      "Loss:  0.8537057042121887\n",
      "------------\n",
      "Period:  1399 of 12000\n",
      "mean performance:  0.07692307692307693\n",
      "mean reward:  0.04\n",
      "Loss:  0.7886941432952881\n",
      "------------\n",
      "Period:  1599 of 12000\n",
      "mean performance:  0.6666666666666666\n",
      "mean reward:  0.2\n",
      "Loss:  0.6651727557182312\n",
      "------------\n",
      "Period:  1799 of 12000\n",
      "mean performance:  0.0\n",
      "mean reward:  0.04\n",
      "Loss:  0.8468347191810608\n",
      "------------\n",
      "Period:  1999 of 12000\n",
      "mean performance:  0.9333333333333333\n",
      "mean reward:  0.28\n",
      "Loss:  0.2608690559864044\n",
      "------------\n",
      "Period:  2199 of 12000\n",
      "mean performance:  0.7857142857142857\n",
      "mean reward:  0.2\n",
      "Loss:  0.5947781801223755\n",
      "------------\n",
      "Period:  2399 of 12000\n",
      "mean performance:  0.2857142857142857\n",
      "mean reward:  0.08\n",
      "Loss:  0.7078301310539246\n",
      "------------\n",
      "Period:  2599 of 12000\n",
      "mean performance:  0.2857142857142857\n",
      "mean reward:  0.1\n",
      "Loss:  0.708930253982544\n",
      "------------\n",
      "Period:  2799 of 12000\n",
      "mean performance:  0.8666666666666667\n",
      "mean reward:  0.22\n",
      "Loss:  0.6125680208206177\n",
      "------------\n",
      "Period:  2999 of 12000\n",
      "mean performance:  0.9230769230769231\n",
      "mean reward:  0.24\n",
      "Loss:  0.5586103796958923\n",
      "------------\n",
      "Period:  3199 of 12000\n",
      "mean performance:  0.15384615384615385\n",
      "mean reward:  0.04\n",
      "Loss:  0.817091703414917\n",
      "------------\n",
      "Period:  3399 of 12000\n",
      "mean performance:  0.2\n",
      "mean reward:  0.12\n",
      "Loss:  0.7549212574958801\n",
      "------------\n",
      "Period:  3599 of 12000\n",
      "mean performance:  0.6428571428571429\n",
      "mean reward:  0.2\n",
      "Loss:  0.6699433922767639\n",
      "------------\n",
      "Period:  3799 of 12000\n",
      "mean performance:  0.6923076923076923\n",
      "mean reward:  0.14\n",
      "Loss:  0.631102442741394\n",
      "------------\n",
      "Period:  3999 of 12000\n",
      "mean performance:  0.46153846153846156\n",
      "mean reward:  0.12\n",
      "Loss:  0.6745988726615906\n",
      "------------\n",
      "Period:  4199 of 12000\n",
      "mean performance:  0.5714285714285714\n",
      "mean reward:  0.18\n",
      "Loss:  0.6942629814147949\n",
      "------------\n",
      "Period:  4399 of 12000\n",
      "mean performance:  0.2\n",
      "mean reward:  0.08\n",
      "Loss:  0.6988421082496643\n",
      "------------\n",
      "Period:  4599 of 12000\n",
      "mean performance:  0.6153846153846154\n",
      "mean reward:  0.14\n",
      "Loss:  0.6965886354446411\n",
      "------------\n",
      "Period:  4799 of 12000\n",
      "mean performance:  0.7142857142857143\n",
      "mean reward:  0.18\n",
      "Loss:  0.6414749622344971\n",
      "------------\n",
      "Period:  4999 of 12000\n",
      "mean performance:  0.7333333333333333\n",
      "mean reward:  0.2\n",
      "Loss:  0.6721403002738953\n",
      "------------\n",
      "Period:  5199 of 12000\n",
      "mean performance:  0.46153846153846156\n",
      "mean reward:  0.12\n",
      "Loss:  0.558428943157196\n",
      "------------\n",
      "Period:  5399 of 12000\n",
      "mean performance:  0.6153846153846154\n",
      "mean reward:  0.12\n",
      "Loss:  0.6695329546928406\n",
      "------------\n",
      "Period:  5599 of 12000\n",
      "mean performance:  0.5\n",
      "mean reward:  0.18\n",
      "Loss:  0.6860361695289612\n",
      "------------\n",
      "Period:  5799 of 12000\n",
      "mean performance:  0.6666666666666666\n",
      "mean reward:  0.18\n",
      "Loss:  0.6702781915664673\n",
      "------------\n",
      "Period:  5999 of 12000\n",
      "mean performance:  0.42857142857142855\n",
      "mean reward:  0.1\n",
      "Loss:  0.7289265394210815\n",
      "------------\n",
      "Period:  6199 of 12000\n",
      "mean performance:  0.5\n",
      "mean reward:  0.12\n",
      "Loss:  0.7606271505355835\n",
      "------------\n",
      "Period:  6399 of 12000\n",
      "mean performance:  0.35714285714285715\n",
      "mean reward:  0.1\n",
      "Loss:  0.6836175322532654\n",
      "------------\n",
      "Period:  6599 of 12000\n",
      "mean performance:  0.0\n",
      "mean reward:  0.06\n",
      "Loss:  0.7701038122177124\n",
      "------------\n",
      "Period:  6799 of 12000\n",
      "mean performance:  0.35714285714285715\n",
      "mean reward:  0.1\n",
      "Loss:  0.700493335723877\n",
      "------------\n",
      "Period:  6999 of 12000\n",
      "mean performance:  0.5714285714285714\n",
      "mean reward:  0.16\n",
      "Loss:  0.6976107954978943\n",
      "------------\n",
      "Period:  7199 of 12000\n",
      "mean performance:  0.6428571428571429\n",
      "mean reward:  0.12\n",
      "Loss:  0.6699397563934326\n",
      "------------\n",
      "Period:  7399 of 12000\n",
      "mean performance:  0.5333333333333333\n",
      "mean reward:  0.14\n",
      "Loss:  0.6953729391098022\n",
      "------------\n",
      "Period:  7599 of 12000\n",
      "mean performance:  0.75\n",
      "mean reward:  0.22\n",
      "Loss:  0.6838400959968567\n",
      "------------\n",
      "Period:  7799 of 12000\n",
      "mean performance:  0.35714285714285715\n",
      "mean reward:  0.1\n",
      "Loss:  0.7137147188186646\n",
      "------------\n",
      "Period:  7999 of 12000\n",
      "mean performance:  0.7692307692307693\n",
      "mean reward:  0.24\n",
      "Loss:  0.6738347411155701\n",
      "------------\n",
      "Period:  8199 of 12000\n",
      "mean performance:  0.42857142857142855\n",
      "mean reward:  0.16\n",
      "Loss:  0.7051042318344116\n",
      "------------\n",
      "Period:  8399 of 12000\n",
      "mean performance:  0.75\n",
      "mean reward:  0.18\n",
      "Loss:  0.645919680595398\n",
      "------------\n",
      "Period:  8599 of 12000\n",
      "mean performance:  0.35714285714285715\n",
      "mean reward:  0.1\n",
      "Loss:  0.690873920917511\n",
      "------------\n",
      "Period:  8799 of 12000\n",
      "mean performance:  0.38461538461538464\n",
      "mean reward:  0.16\n",
      "Loss:  0.6957626342773438\n",
      "------------\n",
      "Period:  8999 of 12000\n",
      "mean performance:  0.9285714285714286\n",
      "mean reward:  0.18\n",
      "Loss:  0.6463181376457214\n",
      "------------\n",
      "Period:  9199 of 12000\n",
      "mean performance:  0.6428571428571429\n",
      "mean reward:  0.2\n",
      "Loss:  0.671528160572052\n",
      "------------\n",
      "Period:  9399 of 12000\n",
      "mean performance:  0.8333333333333334\n",
      "mean reward:  0.18\n",
      "Loss:  0.6844611167907715\n",
      "------------\n",
      "Period:  9599 of 12000\n",
      "mean performance:  0.6153846153846154\n",
      "mean reward:  0.14\n",
      "Loss:  0.7174460291862488\n",
      "------------\n",
      "Period:  9799 of 12000\n",
      "mean performance:  0.3333333333333333\n",
      "mean reward:  0.08\n",
      "Loss:  0.7231249213218689\n",
      "------------\n",
      "Period:  9999 of 12000\n",
      "mean performance:  0.7333333333333333\n",
      "mean reward:  0.22\n",
      "Loss:  0.6234931349754333\n",
      "------------\n",
      "Period:  10199 of 12000\n",
      "mean performance:  0.6153846153846154\n",
      "mean reward:  0.16\n",
      "Loss:  0.6642897725105286\n",
      "------------\n",
      "Period:  10399 of 12000\n",
      "mean performance:  0.21428571428571427\n",
      "mean reward:  0.08\n",
      "Loss:  0.7295598983764648\n",
      "------------\n",
      "Period:  10599 of 12000\n",
      "mean performance:  0.6153846153846154\n",
      "mean reward:  0.18\n",
      "Loss:  0.6256022453308105\n",
      "------------\n",
      "Period:  10799 of 12000\n",
      "mean performance:  0.3076923076923077\n",
      "mean reward:  0.1\n",
      "Loss:  0.7214304208755493\n",
      "------------\n",
      "Period:  10999 of 12000\n",
      "mean performance:  0.5714285714285714\n",
      "mean reward:  0.2\n",
      "Loss:  0.7228518128395081\n",
      "------------\n",
      "Period:  11199 of 12000\n",
      "mean performance:  0.38461538461538464\n",
      "mean reward:  0.08\n",
      "Loss:  0.7492029666900635\n",
      "------------\n",
      "Period:  11399 of 12000\n",
      "mean performance:  0.6923076923076923\n",
      "mean reward:  0.16\n",
      "Loss:  0.6619051098823547\n",
      "------------\n",
      "Period:  11599 of 12000\n",
      "mean performance:  0.2857142857142857\n",
      "mean reward:  0.12\n",
      "Loss:  0.699313223361969\n",
      "------------\n",
      "Period:  11799 of 12000\n",
      "mean performance:  0.35714285714285715\n",
      "mean reward:  0.1\n",
      "Loss:  0.7255452275276184\n",
      "------------\n",
      "Period:  11999 of 12000\n",
      "mean performance:  0.23076923076923078\n",
      "mean reward:  0.08\n",
      "Loss:  0.7629085779190063\n",
      "------------\n",
      "Period:  199 of 12000\n",
      "mean performance:  0.0\n",
      "mean reward:  0.04\n",
      "Loss:  0.74757319688797\n",
      "------------\n",
      "Period:  399 of 12000\n",
      "mean performance:  0.07692307692307693\n",
      "mean reward:  0.02\n",
      "Loss:  0.7106773853302002\n",
      "------------\n",
      "Period:  599 of 12000\n",
      "mean performance:  0.7142857142857143\n",
      "mean reward:  0.18\n",
      "Loss:  0.5508026480674744\n",
      "------------\n",
      "Period:  799 of 12000\n",
      "mean performance:  0.75\n",
      "mean reward:  0.2\n",
      "Loss:  0.6871020793914795\n",
      "------------\n",
      "Period:  999 of 12000\n",
      "mean performance:  0.46153846153846156\n",
      "mean reward:  0.1\n",
      "Loss:  0.6733109951019287\n",
      "------------\n",
      "Period:  1199 of 12000\n",
      "mean performance:  0.7142857142857143\n",
      "mean reward:  0.2\n",
      "Loss:  0.5975167751312256\n",
      "------------\n",
      "Period:  1399 of 12000\n",
      "mean performance:  0.46153846153846156\n",
      "mean reward:  0.16\n",
      "Loss:  0.8049665689468384\n",
      "------------\n",
      "Period:  1599 of 12000\n",
      "mean performance:  0.42857142857142855\n",
      "mean reward:  0.14\n",
      "Loss:  0.7371644973754883\n",
      "------------\n",
      "Period:  1799 of 12000\n",
      "mean performance:  0.46153846153846156\n",
      "mean reward:  0.1\n",
      "Loss:  0.6830577254295349\n",
      "------------\n",
      "Period:  1999 of 12000\n",
      "mean performance:  0.5384615384615384\n",
      "mean reward:  0.16\n",
      "Loss:  0.6608009934425354\n",
      "------------\n",
      "Period:  2199 of 12000\n",
      "mean performance:  0.0\n",
      "mean reward:  0.02\n",
      "Loss:  0.8180254697799683\n",
      "------------\n",
      "Period:  2399 of 12000\n",
      "mean performance:  0.6153846153846154\n",
      "mean reward:  0.16\n",
      "Loss:  0.6684703826904297\n",
      "------------\n",
      "Period:  2599 of 12000\n",
      "mean performance:  0.3333333333333333\n",
      "mean reward:  0.1\n",
      "Loss:  0.7622566819190979\n",
      "------------\n",
      "Period:  2799 of 12000\n",
      "mean performance:  0.7142857142857143\n",
      "mean reward:  0.18\n",
      "Loss:  0.6628832221031189\n",
      "------------\n",
      "Period:  2999 of 12000\n",
      "mean performance:  0.8\n",
      "mean reward:  0.22\n",
      "Loss:  0.6202154755592346\n",
      "------------\n",
      "Period:  3199 of 12000\n",
      "mean performance:  0.5\n",
      "mean reward:  0.12\n",
      "Loss:  0.6634032726287842\n",
      "------------\n",
      "Period:  3399 of 12000\n",
      "mean performance:  0.2857142857142857\n",
      "mean reward:  0.08\n",
      "Loss:  0.7165693044662476\n",
      "------------\n",
      "Period:  3599 of 12000\n",
      "mean performance:  0.2857142857142857\n",
      "mean reward:  0.12\n",
      "Loss:  0.709337055683136\n",
      "------------\n",
      "Period:  3799 of 12000\n",
      "mean performance:  0.6428571428571429\n",
      "mean reward:  0.18\n",
      "Loss:  0.6748933792114258\n",
      "------------\n",
      "Period:  3999 of 12000\n",
      "mean performance:  0.0625\n",
      "mean reward:  0.06\n",
      "Loss:  0.7970753908157349\n",
      "------------\n",
      "Period:  4199 of 12000\n",
      "mean performance:  0.7692307692307693\n",
      "mean reward:  0.22\n",
      "Loss:  0.6381751894950867\n",
      "------------\n",
      "Period:  4399 of 12000\n",
      "mean performance:  0.8571428571428571\n",
      "mean reward:  0.22\n",
      "Loss:  0.6459547877311707\n",
      "------------\n",
      "Period:  4599 of 12000\n",
      "mean performance:  0.46153846153846156\n",
      "mean reward:  0.12\n",
      "Loss:  0.6804897785186768\n",
      "------------\n",
      "Period:  4799 of 12000\n",
      "mean performance:  0.4\n",
      "mean reward:  0.16\n",
      "Loss:  37.68310546875\n",
      "------------\n",
      "Period:  4999 of 12000\n",
      "mean performance:  0.8571428571428571\n",
      "mean reward:  0.24\n",
      "Loss:  29790210.0\n",
      "------------\n",
      "Period:  5199 of 12000\n",
      "mean performance:  0.5384615384615384\n",
      "mean reward:  0.12\n",
      "Loss:  2271946431528960.0\n",
      "------------\n",
      "Period:  5399 of 12000\n",
      "mean performance:  0.7692307692307693\n",
      "mean reward:  0.18\n",
      "Loss:  18722166784.0\n",
      "------------\n",
      "Period:  5599 of 12000\n",
      "mean performance:  0.6923076923076923\n",
      "mean reward:  0.16\n",
      "Loss:  225643069440.0\n",
      "------------\n",
      "Period:  5799 of 12000\n",
      "mean performance:  0.21428571428571427\n",
      "mean reward:  0.06\n",
      "Loss:  231763329024.0\n",
      "------------\n",
      "Period:  5999 of 12000\n",
      "mean performance:  0.5\n",
      "mean reward:  0.16\n",
      "Loss:  79689162752.0\n",
      "------------\n",
      "Period:  6199 of 12000\n",
      "mean performance:  0.46153846153846156\n",
      "mean reward:  0.12\n",
      "Loss:  1158630211584.0\n",
      "------------\n",
      "Period:  6399 of 12000\n",
      "mean performance:  1.0\n",
      "mean reward:  0.26\n",
      "Loss:  39445.1328125\n",
      "------------\n",
      "Period:  6599 of 12000\n",
      "mean performance:  0.7692307692307693\n",
      "mean reward:  0.18\n",
      "Loss:  647577984.0\n",
      "------------\n",
      "Period:  6799 of 12000\n",
      "mean performance:  0.7857142857142857\n",
      "mean reward:  0.22\n",
      "Loss:  4296524.0\n",
      "------------\n",
      "Period:  6999 of 12000\n",
      "mean performance:  0.38461538461538464\n",
      "mean reward:  0.14\n",
      "Loss:  10571825152.0\n",
      "------------\n",
      "Period:  7199 of 12000\n",
      "mean performance:  0.5833333333333334\n",
      "mean reward:  0.14\n",
      "Loss:  2292274432.0\n",
      "------------\n",
      "Period:  7399 of 12000\n",
      "mean performance:  0.4166666666666667\n",
      "mean reward:  0.1\n",
      "Loss:  15024674816.0\n",
      "------------\n",
      "Period:  7599 of 12000\n",
      "mean performance:  0.16666666666666666\n",
      "mean reward:  0.06\n",
      "Loss:  29236070400.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a105edb33d24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                               \u001b[0menv_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                               \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                               criterion=criterion, num_periods=num_periods, seq_len=seq_len)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmperf_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mean_perf_by_seq_len\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_ITI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ITI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfix_dur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_dur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblk_dur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/foragingRNNs/forage_training.py\u001b[0m in \u001b[0;36mtrain_multiple_networks\u001b[0;34m(mean_ITI, fix_dur, blk_dur, num_networks, env, env_seed, main_folder, save_folder, filename, env_kwargs, net_kwargs, criterion, num_periods, seq_len, debug, num_steps_test, num_steps_plot)\u001b[0m\n\u001b[1;32m    618\u001b[0m                                             \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                                             \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                                             save_folder=save_folder_net)\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0;31m# save data as npz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavez\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_folder_net\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/data.npz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata_behav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/foragingRNNs/forage_training.py\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(num_periods, criterion, env, net_kwargs, env_kwargs, seq_len, debug, seed, save_folder)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi_per\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_periods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         data = run_agent_in_environment(env=env, net=net,\n\u001b[0;32m--> 325\u001b[0;31m                                         num_steps_exp=seq_len)\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;31m# transform list of torch to torch tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'act_pr_mat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/foragingRNNs/forage_training.py\u001b[0m in \u001b[0;36mrun_agent_in_environment\u001b[0;34m(num_steps_exp, env, net)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mob_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mob_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mob_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0maction_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mob_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0mact_pr_mat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;31m# Assuming `net` returns action probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/foragingRNNs/forage_training.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     50\u001b[0m                                  self.hidden_size)\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# get the output of the network for a given input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvanilla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 266\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = True\n",
    "# define parameter to explore\n",
    "seq_len_mat = np.array([50, 300, 1000])\n",
    "\n",
    "\n",
    "if train:\n",
    "  for seq_len in seq_len_mat:\n",
    "    num_periods = total_num_timesteps // seq_len\n",
    "    _, _ = train_multiple_networks(mean_ITI=mean_ITI, fix_dur=fix_dur, blk_dur=blk_dur,\n",
    "                              num_networks=num_networks, env=env,\n",
    "                              env_seed=env_seed, main_folder=main_folder, save_folder=save_folder,\n",
    "                              filename=filename, env_kwargs=env_kwargs, net_kwargs=net_kwargs,\n",
    "                              criterion=criterion, num_periods=num_periods, seq_len=seq_len)\n",
    "\n",
    "mperf_lists = get_mean_perf_by_seq_len(main_folder, filename, seq_len_mat, w_factor, mean_ITI, max_ITI, fix_dur, dec_dur, blk_dur, probs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
